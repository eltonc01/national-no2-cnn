{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db760dde",
   "metadata": {},
   "source": [
    "# National NO<sub>2</sub> Convolutional Neural Network Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4003ca9a",
   "metadata": {},
   "source": [
    "In this notebook, we provide an example of a simple train test split using our PyTorch based model architecture. The NO<sub>2</sub> measurements are first obtained from the 'daily measurements.csv' file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6964a052-8923-40c3-906e-3a7dd4a89239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary modules\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Subset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.utils import shuffle\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torchmetrics.functional import r2_score, mean_absolute_error\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f78648-1a4c-4a4a-b7ee-3ce42fc1f85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify cuda capable GPU\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602dddb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_all = pd.read_csv('data/annual measurements.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4200e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "site_nums = df_all['Monitoring Num'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0016ab76-15ba-4ff5-b7ef-168a790fd480",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchStandardScaler:\n",
    "    def fit(self, x, dim):\n",
    "        self.mean = x.mean(dim, keepdim=True)\n",
    "        self.std = x.std(dim, unbiased=False, keepdim=True)\n",
    "    def transform(self, x):\n",
    "        x -= self.mean\n",
    "        x /= (self.std + 1e-5)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99b3620-2c80-49c5-9fb2-ea7888a76397",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder()\n",
    "encoder.fit(df_all['State'].unique().reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6eda1ad",
   "metadata": {},
   "source": [
    "## Load data into memory\n",
    "\n",
    "Next, we load data files into memory. Each file is formatted as a dictionary saved as a NumPy file. Each file loads data at different temporal frequencies. We convert each array to a Tensor and unsqueeze it. After, that we take data from these dictionaries to form the actual samples list, called 'items.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab7952f-4626-4430-8cc2-e4bf7f84b06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "era5 = np.load('data/annual era5.npy', allow_pickle=True).item()\n",
    "omi_no2 = np.load('data/annual no2.npy', allow_pickle=True).item()\n",
    "roas = np.load('data/road images.npy', allow_pickle=True).item()\n",
    "eles = np.load('data/elevation.npy', allow_pickle=True).item()\n",
    "firs = np.load('data/annual fires.npy', allow_pickle=True).item()\n",
    "ndvs = np.load('data/annual ndvi.npy', allow_pickle=True).item()\n",
    "pops = np.load('data/pop density.npy', allow_pickle=True).item()\n",
    "plas = np.load('data/power plants.npy', allow_pickle=True).item()\n",
    "tres = np.load('data/tree cover.npy', allow_pickle=True).item()\n",
    "wats = np.load('data/water.npy', allow_pickle=True).item()\n",
    "wels = np.load('data/wells.npy', allow_pickle=True).item()\n",
    "rais = np.load('data/railway images.npy', allow_pickle=True).item()\n",
    "imps = np.load('data/impervious.npy', allow_pickle=True).item()\n",
    "\n",
    "# one image for all data\n",
    "for num in tqdm(site_nums):\n",
    "    roas[num] = torch.Tensor(np.concatenate(list(roas[num].values())))\n",
    "    eles[num] = torch.Tensor(eles[num]).unsqueeze(dim=0)\n",
    "    wats[num] = torch.Tensor(wats[num]).unsqueeze(dim=0)\n",
    "    pop = torch.Tensor(pops[num]).unsqueeze(dim=0)\n",
    "    pop[pop < 0] = 0\n",
    "    pops[num] = pop\n",
    "    pla = torch.Tensor(np.stack(list(plas[num].values())))\n",
    "    pla[pla > 0] = 1\n",
    "    plas[num] = pla\n",
    "    wels[num] = torch.Tensor(wels[num]).unsqueeze(dim=0)\n",
    "    imps[num] = torch.Tensor(imps[num]).unsqueeze(dim=0)\n",
    "    rais[num] = torch.Tensor(rais[num]).unsqueeze(dim=0)\n",
    "\n",
    "# yearly images\n",
    "for num in tqdm(site_nums):\n",
    "    for year in list(tres[num]):\n",
    "        tres[num][year] = torch.Tensor(tres[num][year])\n",
    "        tres[num][2021] = tres[num][2020]\n",
    "        tres[num][2022] = tres[num][2020]\n",
    "\n",
    "# monthly images\n",
    "for num in tqdm(site_nums):\n",
    "    year_info = firs[num]\n",
    "    for year in list(year_info):\n",
    "        firs[num][year] = torch.Tensor(year_info[year]).unsqueeze(dim=0)\n",
    "    year_info = ndvs[num]\n",
    "    for year in list(year_info):\n",
    "        ndvs[num][year] = torch.Tensor(year_info[year]).unsqueeze(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bff792-3283-43d0-abae-76f7fd4d05a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "\n",
    "for index in tqdm(range(len(df_all))):\n",
    "    coors = np.array([df_all.loc[index, 'Latitude'], df_all.loc[index, 'Longitude']])\n",
    "    state = encoder.transform([[df_all.loc[index, 'State']]]).toarray()[0]\n",
    "    num = df_all.loc[index, 'Monitoring Num']\n",
    "    year = df_all.loc[index, 'Year']\n",
    "    time_data = np.array([year])\n",
    "\n",
    "    road = roas[num]\n",
    "    elevation = eles[num]\n",
    "    fires = firs[num][year]\n",
    "    ndvi = ndvs[num][year]\n",
    "    pop = pops[num]\n",
    "    plants = plas[num]\n",
    "    tree = tres[num][year]\n",
    "    water = wats[num]\n",
    "    wells = wels[num]\n",
    "    rails = rais[num]\n",
    "    impervious = imps[num]\n",
    "    no2 = torch.Tensor(omi_no2[num][year]).unsqueeze(dim=0)\n",
    "\n",
    "    era = era5[num][year]\n",
    "    nums = torch.Tensor(np.concatenate([era, state, coors, time_data])).to(torch.float)\n",
    "\n",
    "    label = torch.Tensor([df_all.loc[index, 'Annual Measurement']]).to(torch.float)\n",
    "\n",
    "    tens = torch.cat([road, elevation, fires, ndvi, no2, pop, plants, tree, water, wells, rails, impervious]).to(torch.float)\n",
    "\n",
    "    dataset.append((tens, nums, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102e12f3-4ce4-4506-a03f-30aa2ac321f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_scaler(dataset, train_indices, dim):\n",
    "    train_img = []\n",
    "    train_num = []\n",
    "    for x in tqdm(train_indices):\n",
    "        samp = dataset[x]\n",
    "        train_img.append(samp[0])\n",
    "        train_num.append(samp[1])\n",
    "\n",
    "    train_img = torch.stack(train_img)\n",
    "    train_num = torch.stack(train_num)\n",
    "\n",
    "    img_scaler = TorchStandardScaler()\n",
    "    img_scaler.fit(train_img, dim)\n",
    "\n",
    "    num_scaler = TorchStandardScaler()\n",
    "    num_scaler.fit(train_num, 0)\n",
    "    \n",
    "    return img_scaler, num_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8395651",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices, test_indices = train_test_split(list(range(len(dataset))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a01d2a7-2e7b-4a07-aa02-31099be467c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_scaler, num_scaler = fit_scaler(dataset, train_indices, dim=(0,2,3))\n",
    "\n",
    "# do not scale for one hot encoded data\n",
    "\n",
    "for num in [10, 11, 12, 13, 18]:\n",
    "    img_scaler.mean[:, num] = 0.00001\n",
    "    img_scaler.std[:, num] = 1.00001\n",
    "\n",
    "for num in range(17, 64):\n",
    "    num_scaler.mean[:, num] = 0.00001\n",
    "    num_scaler.std[:, num] = 1.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbb77d6-5ed4-4f0e-9ff2-20bad974a277",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_data = Subset(dataset, train_indices)\n",
    "test_data = Subset(dataset, test_indices)\n",
    "train_dl = DataLoader(train_data, batch_size, shuffle=True)\n",
    "test_dl = DataLoader(test_data, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c1f602",
   "metadata": {},
   "source": [
    "## Model Architectures\n",
    "\n",
    "To build our model architecture, we first build a base class called RegressionBase in which the primary CNN class will extend. The primary CNN contains two parts: a CNN and a feedforward network. First, the CNN processes the image data. Then, the numerical data is concatenated with the CNN selected features, before being processed into the feedforward network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bd5c94-6b17-42d9-9129-b0d279cb6d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# r2: r-squared score\n",
    "# mae: mean absolute error\n",
    "\n",
    "class RegressionBase(nn.Module):\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        images, nums, labels = batch \n",
    "        labels = labels.to(device)\n",
    "        out = self(img_scaler.transform(images.to(device)), num_scaler.transform(nums.to(device)))\n",
    "        loss = F.mse_loss(out, labels)\n",
    "        acc = r2_score(out, labels)\n",
    "        return loss, acc\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images, nums, labels = batch\n",
    "        labels = labels.to(device)\n",
    "        out = self(img_scaler.transform(images.to(device)), num_scaler.transform(nums.to(device)))\n",
    "        loss = F.mse_loss(out, labels)\n",
    "        acc = r2_score(out, labels)\n",
    "        mae = mean_absolute_error(out, labels)\n",
    "        return loss.detach(), acc, mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6576df9-8a39-4f11-a46d-4ba624208d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(RegressionBase):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.CNN = nn.Sequential(\n",
    "            \n",
    "            nn.Conv2d(21, 32, kernel_size = 3, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "        \n",
    "            nn.Conv2d(64, 128, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            \n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Linear(3907, 1500)\n",
    "        self.dp1  = nn.Dropout(p=0.2)\n",
    "        self.fc2 = nn.Linear(1500, 500)\n",
    "        self.dp2 = nn.Dropout(p=0.5)\n",
    "        self.fc3 = nn.Linear(500,1)\n",
    "    \n",
    "    def forward(self, image, data):\n",
    "        x1 = self.CNN(image)\n",
    "        x2 = data\n",
    "        \n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dp1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dp2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "        \n",
    "model = CNN()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c805b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_dl):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        res = [model.validation_step(b) for b in val_dl]\n",
    "        losses = [x[0] for x in res]\n",
    "        epoch_loss = torch.stack(losses).mean()\n",
    "        accs = [x[1] for x in res]\n",
    "        epoch_acc = torch.stack(accs).mean()\n",
    "        mae = [x[2] for x in res]\n",
    "        epoch_mae = torch.stack(mae).mean()\n",
    "    return {'loss': epoch_loss.item(), 'acc': epoch_acc.item(), 'mae': epoch_mae.item()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5863ebe6-4117-4cc0-8b21-1077b7cd5f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# send scaler to GPU\n",
    "\n",
    "img_scaler.mean = img_scaler.mean.to(device)\n",
    "img_scaler.std = img_scaler.std.to(device)\n",
    "num_scaler.mean = num_scaler.mean.to(device)\n",
    "num_scaler.std = num_scaler.std.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7b299a-69f4-4260-a7a0-6f0abaad98e3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train model\n",
    "\n",
    "lr = 0.0005\n",
    "epochs = 300\n",
    "\n",
    "performances = []\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr, momentum=0.9)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    progress = tqdm(train_dl)\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    for _, batch in enumerate(progress):\n",
    "        loss, acc = model.training_step(batch)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        train_losses.append(loss.item())\n",
    "        train_accs.append(acc.item())\n",
    "        \n",
    "        progress.set_description(f'Epoch [{epoch}/{epochs}]')\n",
    "        progress.set_postfix(loss=np.mean(train_losses), acc=np.mean(train_accs))\n",
    "        \n",
    "\n",
    "    result = evaluate(model, test_dl)\n",
    "    result['train loss'] = np.mean(train_losses)\n",
    "    performances.append(result)\n",
    "    \n",
    "    print(\"Epoch [{}/{}], train loss: {:.4f}, val loss: {:.4f}, acc: {:.4f}, mae: {:.4f}\".format(epoch, epochs, result['train loss'], result['loss'], result['acc'], result['mae']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
